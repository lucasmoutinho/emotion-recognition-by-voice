{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/.pyenv/versions/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gabriel/.pyenv/versions/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gabriel/.pyenv/versions/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gabriel/.pyenv/versions/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gabriel/.pyenv/versions/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gabriel/.pyenv/versions/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/gabriel/.pyenv/versions/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gabriel/.pyenv/versions/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gabriel/.pyenv/versions/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gabriel/.pyenv/versions/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gabriel/.pyenv/versions/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gabriel/.pyenv/versions/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modules.dataset_loader import DatasetLoader\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(X,y):\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    max_len = len(X[0])\n",
    "    for row in X:\n",
    "        if len(row) > max_len:\n",
    "            max_len = len(row)\n",
    "\n",
    "    X = pad_sequences(X, maxlen=max_len, padding='post', dtype='float64')\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    DATASET_PATH = 'datasets/Original/MFCC/'\n",
    "    dataset_loader = DatasetLoader(DATASET_PATH)\n",
    "    mfcc_features, y = dataset_loader.get_dataset()\n",
    "\n",
    "    DATASET_PATH = 'datasets/Original/Prosody/'\n",
    "    dataset_loader = DatasetLoader(DATASET_PATH)\n",
    "    prosody_features, y = dataset_loader.get_dataset()\n",
    "\n",
    "\n",
    "    new_dataset = []\n",
    "    for index in range(0, len(mfcc_features)):\n",
    "        new_instance = []\n",
    "        for row_index in range(0, len(mfcc_features[index])):\n",
    "            new_row = np.concatenate(\n",
    "                (mfcc_features[index][row_index],\n",
    "                prosody_features[index][row_index]),\n",
    "                axis= None\n",
    "            )\n",
    "            new_instance.append(new_row)\n",
    "        new_dataset.append(new_instance)\n",
    "\n",
    "    X = new_dataset\n",
    "\n",
    "    DATASET_PATH = 'datasets/Original/Chroma/'\n",
    "    dataset_loader = DatasetLoader(DATASET_PATH)\n",
    "    chroma_features, y = dataset_loader.get_dataset()\n",
    "\n",
    "    new_dataset = []\n",
    "    for index in range(0, len(chroma_features)):\n",
    "        new_instance = []\n",
    "        for row_index in range(0, len(chroma_features[index])):\n",
    "            new_row = np.concatenate(\n",
    "                (X[index][row_index],\n",
    "                chroma_features[index][row_index]),\n",
    "                axis= None\n",
    "            )\n",
    "            new_instance.append(new_row)\n",
    "        new_dataset.append(new_instance)\n",
    "\n",
    "    X = np.asarray(new_dataset)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_mapping =  {'neu': 0,\n",
    "            'des': 1,\n",
    "            'med': 2,\n",
    "            'ale': 3,\n",
    "            'rai': 4,\n",
    "            'sur': 5,\n",
    "            'tri': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_file = [] \n",
    "results_dir = os.listdir('best_hyperopt_results')\n",
    "for filename in results_dir:\n",
    "    if '.h5' in filename:\n",
    "        models_file.append(\"best_hyperopt_results/\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_hyperopt_results/tri.h5',\n",
       " 'best_hyperopt_results/des.h5',\n",
       " 'best_hyperopt_results/ale.h5',\n",
       " 'best_hyperopt_results/sur.h5',\n",
       " 'best_hyperopt_results/rai.h5',\n",
       " 'best_hyperopt_results/neu.h5',\n",
       " 'best_hyperopt_results/med.h5']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "models = dict()\n",
    "for model_file in models_file:\n",
    "    models[model_file.split('/')[-1][:-3]] = keras.models.load_model(model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tri': <tensorflow.python.keras.engine.sequential.Sequential at 0x7ff89b768668>,\n",
       " 'des': <tensorflow.python.keras.engine.sequential.Sequential at 0x7ff8984c9048>,\n",
       " 'ale': <tensorflow.python.keras.engine.sequential.Sequential at 0x7ff89b7689b0>,\n",
       " 'sur': <tensorflow.python.keras.engine.sequential.Sequential at 0x7ff89b739898>,\n",
       " 'rai': <tensorflow.python.keras.engine.sequential.Sequential at 0x7ff89b7682e8>,\n",
       " 'neu': <tensorflow.python.keras.engine.sequential.Sequential at 0x7ff88049e940>,\n",
       " 'med': <tensorflow.python.keras.engine.sequential.Sequential at 0x7ff8666f9b38>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = DatasetLoader()\n",
    "X,y = dataset_loader.compose_complete_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = [ins]\n",
    "y = [inst[0] for inst in y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = add_padding(X,y)\n",
    "y = to_categorical(y)\n",
    "num_rows = X[0].shape[0]\n",
    "num_columns = X[0].shape[1]\n",
    "num_channels = 1\n",
    "X = X.reshape(X.shape[0], num_rows, num_columns, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dict()                                                   \n",
    "for model in models.keys():                                            \n",
    "    predictions[model] = models[model].predict(X)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "YES_INDEX = 1\n",
    "NO_INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [emotion for emotion in predictions.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = [insty[0] for insty in insty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = np.asarray(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neu': 0, 'des': 1, 'med': 2, 'ale': 3, 'rai': 4, 'sur': 5, 'tri': 6}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 2.000000e+00],\n",
       "         [ 2.000000e-02],\n",
       "         [-1.544287e+01],\n",
       "         ...,\n",
       "         [ 1.011527e-01],\n",
       "         [ 7.180860e-02],\n",
       "         [ 8.352990e-02]],\n",
       "\n",
       "        [[ 3.000000e+00],\n",
       "         [ 3.000000e-02],\n",
       "         [-1.647763e+01],\n",
       "         ...,\n",
       "         [ 9.720979e-02],\n",
       "         [ 4.277883e-02],\n",
       "         [ 4.137888e-02]],\n",
       "\n",
       "        [[ 4.000000e+00],\n",
       "         [ 4.000000e-02],\n",
       "         [-1.847457e+01],\n",
       "         ...,\n",
       "         [ 7.305314e-02],\n",
       "         [ 6.022261e-02],\n",
       "         [ 5.212764e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]],\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]],\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 2.000000e+00],\n",
       "         [ 2.000000e-02],\n",
       "         [-1.606690e+01],\n",
       "         ...,\n",
       "         [ 1.072428e-01],\n",
       "         [ 8.151104e-02],\n",
       "         [ 3.828605e-02]],\n",
       "\n",
       "        [[ 3.000000e+00],\n",
       "         [ 3.000000e-02],\n",
       "         [-1.877527e+01],\n",
       "         ...,\n",
       "         [ 1.152646e-01],\n",
       "         [ 7.592449e-02],\n",
       "         [ 3.994263e-02]],\n",
       "\n",
       "        [[ 4.000000e+00],\n",
       "         [ 4.000000e-02],\n",
       "         [-1.525225e+01],\n",
       "         ...,\n",
       "         [ 1.008192e-01],\n",
       "         [ 7.029983e-02],\n",
       "         [ 7.502092e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]],\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]],\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 2.000000e+00],\n",
       "         [ 2.000000e-02],\n",
       "         [ 8.439281e+00],\n",
       "         ...,\n",
       "         [ 2.128077e-02],\n",
       "         [ 3.594450e-02],\n",
       "         [ 9.712623e-03]],\n",
       "\n",
       "        [[ 3.000000e+00],\n",
       "         [ 3.000000e-02],\n",
       "         [ 9.104332e+00],\n",
       "         ...,\n",
       "         [ 3.412940e-02],\n",
       "         [ 5.643203e-02],\n",
       "         [ 9.262300e-03]],\n",
       "\n",
       "        [[ 4.000000e+00],\n",
       "         [ 4.000000e-02],\n",
       "         [ 1.010406e+01],\n",
       "         ...,\n",
       "         [ 3.602191e-02],\n",
       "         [ 6.052217e-02],\n",
       "         [ 1.453083e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]],\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]],\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 2.000000e+00],\n",
       "         [ 2.000000e-02],\n",
       "         [-1.472218e+01],\n",
       "         ...,\n",
       "         [ 8.985269e-02],\n",
       "         [ 6.432172e-02],\n",
       "         [ 6.024454e-02]],\n",
       "\n",
       "        [[ 3.000000e+00],\n",
       "         [ 3.000000e-02],\n",
       "         [-1.191806e+01],\n",
       "         ...,\n",
       "         [ 4.566583e-02],\n",
       "         [ 4.583181e-02],\n",
       "         [ 1.002767e-01]],\n",
       "\n",
       "        [[ 4.000000e+00],\n",
       "         [ 4.000000e-02],\n",
       "         [-6.074732e+00],\n",
       "         ...,\n",
       "         [ 2.923499e-02],\n",
       "         [ 2.687966e-02],\n",
       "         [ 6.483422e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]],\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]],\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 2.000000e+00],\n",
       "         [ 2.000000e-02],\n",
       "         [ 8.497627e+00],\n",
       "         ...,\n",
       "         [ 2.308322e-02],\n",
       "         [ 6.166708e-02],\n",
       "         [ 7.599541e-03]],\n",
       "\n",
       "        [[ 3.000000e+00],\n",
       "         [ 3.000000e-02],\n",
       "         [ 6.279279e+00],\n",
       "         ...,\n",
       "         [ 3.269858e-02],\n",
       "         [ 6.388778e-02],\n",
       "         [ 9.381620e-03]],\n",
       "\n",
       "        [[ 4.000000e+00],\n",
       "         [ 4.000000e-02],\n",
       "         [ 2.027337e+00],\n",
       "         ...,\n",
       "         [ 4.414143e-02],\n",
       "         [ 5.269538e-02],\n",
       "         [ 1.446362e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]],\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]],\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]]],\n",
       "\n",
       "\n",
       "       [[[ 2.000000e+00],\n",
       "         [ 2.000000e-02],\n",
       "         [ 1.138831e+01],\n",
       "         ...,\n",
       "         [ 7.471852e-02],\n",
       "         [ 1.509907e-01],\n",
       "         [ 9.359862e-02]],\n",
       "\n",
       "        [[ 3.000000e+00],\n",
       "         [ 3.000000e-02],\n",
       "         [ 9.949474e+00],\n",
       "         ...,\n",
       "         [ 2.691960e-02],\n",
       "         [ 8.113650e-02],\n",
       "         [ 1.080007e-01]],\n",
       "\n",
       "        [[ 4.000000e+00],\n",
       "         [ 4.000000e-02],\n",
       "         [ 9.484652e+00],\n",
       "         ...,\n",
       "         [ 1.141259e-02],\n",
       "         [ 3.303066e-02],\n",
       "         [ 6.586621e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]],\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]],\n",
       "\n",
       "        [[ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         ...,\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00],\n",
       "         [ 0.000000e+00]]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[np.where(y_labels==5)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise_results = models['sur'].predict(X[np.where(y_labels==0)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sresults = [np.argmax(result) for result in surprise_results]\n",
    "sresults_2 = [el for el in map(lambda x: \"Hit\" if x == 1 else \"Miss\", sresults)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(np.asarray(sresults_2)=='Hit')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " predictions = dict()                                                   \n",
    " for model in models.keys():                                            \n",
    "    predictions[model] = models[model].predict(X)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit ('machine-learning': venv)",
   "language": "python",
   "name": "python36464bitmachinelearningvenv22b173e40fb14778a7d876c9bd339153"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
