{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and program variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/moutinho/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/moutinho/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/moutinho/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/moutinho/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/moutinho/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/moutinho/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/moutinho/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/moutinho/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/moutinho/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/moutinho/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/moutinho/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/moutinho/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from time_series_dataset_loader import TimeSeriesDatasetLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split # to split dataset into train and test\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../datasets/MFCC/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = TimeSeriesDatasetLoader(DATASET_PATH)\n",
    "X, y = dataset_loader.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.          0.         -7.33182    19.48468    -7.506955    1.82857\n",
      "   5.773757  -13.56968     3.013354  -10.19034    13.51374    -1.628919\n",
      "   7.978467    0.7820195   2.435657   -2.008263 ]\n",
      "(1066,)\n",
      "[3 0 1]\n",
      "(1066,)\n"
     ]
    }
   ],
   "source": [
    "# See X and y details\n",
    "print(X[0][0])\n",
    "print(X.shape)\n",
    "\n",
    "print(y[:3])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0  -7  19  -7   1   5 -13   3 -10  13  -1   7   0   2  -2]\n",
      "(1066, 543, 16)\n"
     ]
    }
   ],
   "source": [
    "# Apply Pad Sequences\n",
    "max_len = len(X[0])\n",
    "for row in X:\n",
    "    if len(row) > max_len:\n",
    "        max_len = len(row)\n",
    "\n",
    "X = pad_sequences(X, maxlen=max_len, padding='post')\n",
    "\n",
    "# See X details\n",
    "print(X[0][0])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train:\n",
      "\n",
      "[  0   0  13  -7  -6  -3   9   1  20   8   1  -2  -8  -6  -7 -12]\n",
      "(746, 543, 16)\n",
      "\n",
      "X_test:\n",
      "\n",
      "[  0   0   6   8   7 -10  -7   3  -2  13  -9   0   5   5   4 -16]\n",
      "(320, 543, 16)\n",
      "\n",
      "y_train:\n",
      "\n",
      "[2 2 5]\n",
      "(746,)\n",
      "\n",
      "y_test:\n",
      "\n",
      "[5 2 0]\n",
      "(320,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3)\n",
    "\n",
    "print(\"\\nX_train:\\n\")\n",
    "print(X_train[0][0])\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"\\nX_test:\\n\")\n",
    "print(X_test[0][0])\n",
    "print(X_test.shape)\n",
    "\n",
    "print(\"\\ny_train:\\n\")\n",
    "print(y_train[:3])\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"\\ny_test:\\n\")\n",
    "print(y_test[:3])\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746\n",
      "543\n",
      "16\n",
      "\n",
      "X_train:\n",
      "\n",
      "(746, 8688)\n",
      "\n",
      "y_train:\n",
      "\n",
      "(746,)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping to apply smote\n",
    "\n",
    "shape_0 = X_train.shape[0]\n",
    "shape_1 = X_train.shape[1]\n",
    "shape_2 = X_train.shape[2]\n",
    "print(shape_0)\n",
    "print(shape_1)\n",
    "print(shape_2)\n",
    "X_train = X_train.reshape(shape_0, shape_1 * shape_2)\n",
    "\n",
    "print(\"\\nX_train:\\n\")\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"\\ny_train:\\n\")\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train:\n",
      "\n",
      "[  0   0  13  -7  -6  -3   9   1  20   8   1  -2  -8  -6  -7 -12]\n",
      "(798, 543, 16)\n",
      "\n",
      "y_train:\n",
      "\n",
      "[2 2 5]\n",
      "(798,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE\n",
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)\n",
    "\n",
    "# Reshaping back to original shape dimensions 1 and 2\n",
    "X_train = X_train.reshape(X_train.shape[0], shape_1, shape_2)\n",
    "\n",
    "print(\"\\nX_train:\\n\")\n",
    "print(X_train[0][0])\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"\\ny_train:\\n\")\n",
    "print(y_train[:3])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical matrices\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train:\n",
      "\n",
      "[[  0]\n",
      " [  0]\n",
      " [ 13]\n",
      " [ -7]\n",
      " [ -6]\n",
      " [ -3]\n",
      " [  9]\n",
      " [  1]\n",
      " [ 20]\n",
      " [  8]\n",
      " [  1]\n",
      " [ -2]\n",
      " [ -8]\n",
      " [ -6]\n",
      " [ -7]\n",
      " [-12]]\n",
      "(798, 543, 16, 1)\n",
      "\n",
      "X_test:\n",
      "\n",
      "[[  0]\n",
      " [  0]\n",
      " [  6]\n",
      " [  8]\n",
      " [  7]\n",
      " [-10]\n",
      " [ -7]\n",
      " [  3]\n",
      " [ -2]\n",
      " [ 13]\n",
      " [ -9]\n",
      " [  0]\n",
      " [  5]\n",
      " [  5]\n",
      " [  4]\n",
      " [-16]]\n",
      "(320, 543, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "num_rows = X[0].shape[0]\n",
    "num_columns = X[0].shape[1]\n",
    "num_channels = 1\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], num_rows, num_columns, num_channels)\n",
    "X_test = X_test.reshape(X_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "print(\"\\nX_train:\\n\")\n",
    "print(X_train[0][0])\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"\\nX_test:\\n\")\n",
    "print(X_test[0][0])\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/moutinho/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct model 1\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-k category accuracy\n",
    "import functools\n",
    "import keras\n",
    "top3_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=3)\n",
    "top3_acc.__name__ = 'top3_acc'\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',top3_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/moutinho/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 798 samples, validate on 320 samples\n",
      "Epoch 1/400\n",
      "798/798 [==============================] - 8s 10ms/step - loss: 3.8604 - accuracy: 0.1466 - top3_acc: 0.4273 - val_loss: 2.2623 - val_accuracy: 0.1562 - val_top3_acc: 0.4406\n",
      "Epoch 2/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 2.7769 - accuracy: 0.1591 - top3_acc: 0.4373 - val_loss: 2.2594 - val_accuracy: 0.1625 - val_top3_acc: 0.4656\n",
      "Epoch 3/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 2.2753 - accuracy: 0.1554 - top3_acc: 0.4511 - val_loss: 2.0098 - val_accuracy: 0.1219 - val_top3_acc: 0.4375\n",
      "Epoch 4/400\n",
      "798/798 [==============================] - 5s 7ms/step - loss: 2.1785 - accuracy: 0.1604 - top3_acc: 0.4549 - val_loss: 2.0652 - val_accuracy: 0.1156 - val_top3_acc: 0.4125\n",
      "Epoch 5/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 2.1440 - accuracy: 0.1491 - top3_acc: 0.4499 - val_loss: 2.0242 - val_accuracy: 0.1219 - val_top3_acc: 0.4094\n",
      "Epoch 6/400\n",
      "798/798 [==============================] - 5s 7ms/step - loss: 2.0329 - accuracy: 0.1441 - top3_acc: 0.4549 - val_loss: 1.9769 - val_accuracy: 0.2156 - val_top3_acc: 0.4437\n",
      "Epoch 7/400\n",
      "798/798 [==============================] - 5s 7ms/step - loss: 1.9592 - accuracy: 0.1767 - top3_acc: 0.4737 - val_loss: 2.0100 - val_accuracy: 0.1625 - val_top3_acc: 0.4437\n",
      "Epoch 8/400\n",
      "798/798 [==============================] - 5s 7ms/step - loss: 1.9893 - accuracy: 0.1779 - top3_acc: 0.4549 - val_loss: 2.0183 - val_accuracy: 0.1625 - val_top3_acc: 0.4313\n",
      "Epoch 9/400\n",
      "798/798 [==============================] - 5s 7ms/step - loss: 1.9808 - accuracy: 0.1491 - top3_acc: 0.4887 - val_loss: 2.0138 - val_accuracy: 0.1656 - val_top3_acc: 0.4250\n",
      "Epoch 10/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9693 - accuracy: 0.1591 - top3_acc: 0.4536 - val_loss: 1.9907 - val_accuracy: 0.1719 - val_top3_acc: 0.4437\n",
      "Epoch 11/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9512 - accuracy: 0.1805 - top3_acc: 0.4862 - val_loss: 1.9770 - val_accuracy: 0.1562 - val_top3_acc: 0.4062\n",
      "Epoch 12/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9664 - accuracy: 0.1754 - top3_acc: 0.4799 - val_loss: 1.9811 - val_accuracy: 0.1625 - val_top3_acc: 0.4187\n",
      "Epoch 13/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9477 - accuracy: 0.1579 - top3_acc: 0.4875 - val_loss: 2.0061 - val_accuracy: 0.1656 - val_top3_acc: 0.4156\n",
      "Epoch 14/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9682 - accuracy: 0.1479 - top3_acc: 0.4737 - val_loss: 1.9927 - val_accuracy: 0.1656 - val_top3_acc: 0.4375\n",
      "Epoch 15/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9474 - accuracy: 0.1391 - top3_acc: 0.4712 - val_loss: 1.9509 - val_accuracy: 0.1625 - val_top3_acc: 0.4531\n",
      "Epoch 16/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9558 - accuracy: 0.1629 - top3_acc: 0.4799 - val_loss: 1.9580 - val_accuracy: 0.1625 - val_top3_acc: 0.4344\n",
      "Epoch 17/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9406 - accuracy: 0.1604 - top3_acc: 0.4812 - val_loss: 1.9910 - val_accuracy: 0.1625 - val_top3_acc: 0.4094\n",
      "Epoch 18/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9542 - accuracy: 0.1516 - top3_acc: 0.4536 - val_loss: 1.9797 - val_accuracy: 0.1656 - val_top3_acc: 0.4375\n",
      "Epoch 19/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9364 - accuracy: 0.1579 - top3_acc: 0.4674 - val_loss: 1.9642 - val_accuracy: 0.1656 - val_top3_acc: 0.4313\n",
      "Epoch 20/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9459 - accuracy: 0.1817 - top3_acc: 0.4799 - val_loss: 1.9625 - val_accuracy: 0.1594 - val_top3_acc: 0.4469\n",
      "Epoch 21/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9299 - accuracy: 0.1779 - top3_acc: 0.4950 - val_loss: 1.9715 - val_accuracy: 0.1594 - val_top3_acc: 0.4062\n",
      "Epoch 22/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9296 - accuracy: 0.1579 - top3_acc: 0.4950 - val_loss: 1.9895 - val_accuracy: 0.1625 - val_top3_acc: 0.4125\n",
      "Epoch 23/400\n",
      "798/798 [==============================] - 6s 8ms/step - loss: 1.9329 - accuracy: 0.1566 - top3_acc: 0.4850 - val_loss: 1.9747 - val_accuracy: 0.1594 - val_top3_acc: 0.4125\n",
      "Epoch 24/400\n",
      "798/798 [==============================] - 6s 8ms/step - loss: 1.9284 - accuracy: 0.1729 - top3_acc: 0.5075 - val_loss: 1.9646 - val_accuracy: 0.1625 - val_top3_acc: 0.4812\n",
      "Epoch 25/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9301 - accuracy: 0.1867 - top3_acc: 0.4812 - val_loss: 1.9631 - val_accuracy: 0.1594 - val_top3_acc: 0.4250\n",
      "Epoch 26/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9316 - accuracy: 0.1867 - top3_acc: 0.4900 - val_loss: 1.9576 - val_accuracy: 0.1625 - val_top3_acc: 0.4500\n",
      "Epoch 27/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9184 - accuracy: 0.1717 - top3_acc: 0.4987 - val_loss: 1.9587 - val_accuracy: 0.1656 - val_top3_acc: 0.4469\n",
      "Epoch 28/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9412 - accuracy: 0.1416 - top3_acc: 0.4825 - val_loss: 1.9451 - val_accuracy: 0.1594 - val_top3_acc: 0.4344\n",
      "Epoch 29/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9229 - accuracy: 0.1679 - top3_acc: 0.5113 - val_loss: 1.9513 - val_accuracy: 0.1250 - val_top3_acc: 0.4500\n",
      "Epoch 30/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9382 - accuracy: 0.1880 - top3_acc: 0.4950 - val_loss: 1.9657 - val_accuracy: 0.1719 - val_top3_acc: 0.4594\n",
      "Epoch 31/400\n",
      "798/798 [==============================] - 6s 8ms/step - loss: 1.9163 - accuracy: 0.1842 - top3_acc: 0.5138 - val_loss: 1.9738 - val_accuracy: 0.1656 - val_top3_acc: 0.4375\n",
      "Epoch 32/400\n",
      "798/798 [==============================] - 6s 8ms/step - loss: 1.9301 - accuracy: 0.1604 - top3_acc: 0.5100 - val_loss: 1.9560 - val_accuracy: 0.1750 - val_top3_acc: 0.4219\n",
      "Epoch 33/400\n",
      "798/798 [==============================] - 6s 8ms/step - loss: 1.9151 - accuracy: 0.1980 - top3_acc: 0.5301 - val_loss: 1.9408 - val_accuracy: 0.1656 - val_top3_acc: 0.4469\n",
      "Epoch 34/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9007 - accuracy: 0.1917 - top3_acc: 0.5213 - val_loss: 1.9408 - val_accuracy: 0.1656 - val_top3_acc: 0.4844\n",
      "Epoch 35/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9131 - accuracy: 0.1830 - top3_acc: 0.5100 - val_loss: 1.9430 - val_accuracy: 0.1656 - val_top3_acc: 0.5063\n",
      "Epoch 36/400\n",
      "798/798 [==============================] - 6s 8ms/step - loss: 1.9171 - accuracy: 0.1805 - top3_acc: 0.5251 - val_loss: 1.9545 - val_accuracy: 0.1625 - val_top3_acc: 0.4625\n",
      "Epoch 37/400\n",
      "798/798 [==============================] - 6s 7ms/step - loss: 1.9080 - accuracy: 0.1779 - top3_acc: 0.5038 - val_loss: 1.9588 - val_accuracy: 0.1688 - val_top3_acc: 0.4469\n",
      "Epoch 38/400\n",
      "256/798 [========>.....................] - ETA: 3s - loss: 1.8976 - accuracy: 0.2109 - top3_acc: 0.5117"
     ]
    }
   ],
   "source": [
    "# Define bath and epochs\n",
    "batch_size = 256\n",
    "epochs = 400\n",
    "\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=20, min_lr=0.0000001)\n",
    "mcp_save = ModelCheckpoint('../models/model_checkpoints/smote_test.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "cnnhistory=model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, y_test), callbacks=[mcp_save, lr_reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model Summary\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# PLT History info\n",
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
