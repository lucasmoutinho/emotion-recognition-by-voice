{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and program variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split # to split dataset into train and test\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../src/Features/Original/MFCC_2/'\n",
    "from time_series_dataset_loader import TimeSeriesDatasetLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = TimeSeriesDatasetLoader(DATASET_PATH)\n",
    "X, y = dataset_loader.get_dataset(type_='emotion_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  0, ...,  0, -1,  1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completing with zeros\n",
    "max_len = len(X[0])\n",
    "for row in X:\n",
    "        if len(row) > max_len:\n",
    "            max_len = len(row)\n",
    "\n",
    "X = pad_sequences(X, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0   0 -16 ...  10   0 -11]\n",
      "  [  1   0 -11 ...  13   4  -2]\n",
      "  [  2   0  -6 ...  11   5   2]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]\n",
      "\n",
      " [[  0   0  -2 ...   8   1 -17]\n",
      "  [  1   0  -1 ...   8   0 -19]\n",
      "  [  2   0   0 ...   8   2 -20]\n",
      "  ...\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]\n",
      "  [  0   0   0 ...   0   0   0]]]\n",
      "(1164, 269, 16)\n",
      "[ 1 -1  0]\n",
      "(1164,)\n"
     ]
    }
   ],
   "source": [
    "# See X and y details\n",
    "print(X[:2])\n",
    "print(X.shape)\n",
    "\n",
    "print(y[:3])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train:\n",
      "\n",
      "[  0   0   0 -13 -12   0  16  16  -1  -1  -7  -2  -8   0   0  12]\n",
      "(814, 269, 16)\n",
      "\n",
      "X_test:\n",
      "\n",
      "[  0   0   2  -4  11   9  13   7   5   5  -4  -1  10   3  -2 -14]\n",
      "(350, 269, 16)\n",
      "\n",
      "y_train:\n",
      "\n",
      "[0 2 6]\n",
      "(814,)\n",
      "\n",
      "y_test:\n",
      "\n",
      "[2 3 2]\n",
      "(350,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3)\n",
    "\n",
    "print(\"\\nX_train:\\n\")\n",
    "print(X_train[0][0])\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"\\nX_test:\\n\")\n",
    "print(X_test[0][0])\n",
    "print(X_test.shape)\n",
    "\n",
    "print(\"\\ny_train:\\n\")\n",
    "print(y_train[:3])\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"\\ny_test:\\n\")\n",
    "print(y_test[:3])\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical matrices\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train:\n",
      "\n",
      "[[  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [-13]\n",
      " [-12]\n",
      " [  0]\n",
      " [ 16]\n",
      " [ 16]\n",
      " [ -1]\n",
      " [ -1]\n",
      " [ -7]\n",
      " [ -2]\n",
      " [ -8]\n",
      " [  0]\n",
      " [  0]\n",
      " [ 12]]\n",
      "(814, 269, 16, 1)\n",
      "\n",
      "X_test:\n",
      "\n",
      "[[  0]\n",
      " [  0]\n",
      " [  2]\n",
      " [ -4]\n",
      " [ 11]\n",
      " [  9]\n",
      " [ 13]\n",
      " [  7]\n",
      " [  5]\n",
      " [  5]\n",
      " [ -4]\n",
      " [ -1]\n",
      " [ 10]\n",
      " [  3]\n",
      " [ -2]\n",
      " [-14]]\n",
      "(350, 269, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "num_rows = X[0].shape[0]\n",
    "num_columns = X[0].shape[1]\n",
    "num_channels = 1\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], num_rows, num_columns, num_channels)\n",
    "X_test = X_test.reshape(X_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "print(\"\\nX_train:\\n\")\n",
    "print(X_train[0][0])\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"\\nX_test:\\n\")\n",
    "print(X_test[0][0])\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, 3, 3, 6, 4, 3, 5, 3, 6, 3, 0, 0, 3, 1, 6, 4, 5, 3, 2, 6,\n",
       "       1, 0, 2, 3, 1, 6, 4, 3, 5, 6, 6, 6, 3, 5, 2, 2, 3, 4, 0, 1, 4, 3,\n",
       "       4, 2, 2, 6, 4, 5, 2, 1, 6, 1, 0, 5, 5, 6, 6, 6, 1, 0, 4, 4, 2, 1,\n",
       "       1, 1, 3, 2, 3, 3, 1, 1, 0, 3, 2, 2, 6, 4, 3, 1, 3, 2, 2, 6, 6, 4,\n",
       "       4, 5, 2, 0, 5, 2, 3, 1, 1, 0, 6, 0, 4, 3, 5, 4, 6, 2, 1, 1, 6, 3,\n",
       "       0, 2, 6, 4, 1, 1, 0, 0, 6, 3, 3, 2, 0, 5, 3, 0, 5, 5, 5, 6, 6, 4,\n",
       "       5, 3, 5, 1, 5, 6, 1, 5, 5, 6, 2, 6, 6, 0, 2, 3, 2, 3, 0, 3, 5, 4,\n",
       "       2, 2, 5, 1, 5, 2, 6, 2, 3, 5, 3, 2, 1, 2, 5, 4, 3, 1, 4, 4, 5, 5,\n",
       "       3, 1, 4, 3, 0, 3, 2, 4, 2, 6, 4, 6, 1, 5, 0, 4, 3, 6, 4, 4, 5, 5,\n",
       "       4, 3, 1, 6, 4, 5, 2, 1, 4, 6, 3, 0, 2, 2, 5, 5, 5, 6, 2, 3, 3, 4,\n",
       "       3, 6, 5, 1, 1, 4, 3, 5, 2, 3, 0, 3, 3, 4, 2, 3, 1, 0, 2, 2, 2, 2,\n",
       "       0, 1, 2, 5, 5, 0, 0, 2, 4, 5, 5, 3, 6, 1, 4, 1, 6, 4, 2, 5, 4, 5,\n",
       "       6, 0, 3, 4, 3, 4, 1, 2, 6, 2, 3, 5, 2, 1, 4, 3, 1, 3, 2, 0, 4, 6,\n",
       "       1, 1, 6, 3, 4, 3, 3, 4, 6, 5, 4, 1, 4, 0, 3, 4, 3, 2, 0, 3, 4, 3,\n",
       "       2, 2, 2, 4, 4, 2, 6, 6, 2, 0, 0, 6, 3, 0, 2, 2, 0, 5, 0, 2, 5, 2,\n",
       "       3, 2, 0, 1, 0, 0, 1, 6, 6, 4, 0, 0, 1, 3, 1, 3, 3, 4, 4, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model 1\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "#     model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-k category accuracy\n",
    "import functools\n",
    "import keras\n",
    "top3_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=3)\n",
    "top3_acc.__name__ = 'top3_acc'\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',top3_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_3 to have shape (7,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-be42436aa98d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlr_reduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0000001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmcp_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcnnhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmcp_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_reduce\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have shape (7,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# Define bath and epochs\n",
    "batch_size = 256\n",
    "epochs = 3\n",
    "checkpoint_file = '../models/model_checkpoints/original_window_2.h5'\n",
    "\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=20, min_lr=0.0000001)\n",
    "mcp_save = ModelCheckpoint(checkpoint_file, save_best_only=True, monitor='val_loss', mode='min')\n",
    "cnnhistory=model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, y_test), callbacks=[mcp_save, lr_reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnhistory.history['accuracy'][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnhistory.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnhistory.history['accuracy'].index(np.sort(cnnhistory.history['accuracy'])[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model Summary\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# PLT History info\n",
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
