{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and program variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split # to split dataset into train and test\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from sklearn import preprocessing \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from time_series_dataset_loader import TimeSeriesDatasetLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretization(X):\n",
    "    new_dataset = []\n",
    "    for data_instance in X:\n",
    "        new_instance = []\n",
    "        for row in data_instance:\n",
    "            new_instance.append(np.average(row[2:]))\n",
    "        new_dataset.append(new_instance)\n",
    "        \n",
    "    return np.asarray(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearize_dataset(dataset):\n",
    "    ''' Returns a 1 dimensional representation from the dataset '''\n",
    "    new_dataset = []\n",
    "\n",
    "    for data_instance in dataset:\n",
    "        new_dataset_cell = []\n",
    "        for t_instance in data_instance:\n",
    "            for value in t_instance[2:]:\n",
    "                new_dataset_cell.append(value)\n",
    "\n",
    "        new_dataset.append(new_dataset_cell)\n",
    "    return np.asarray(new_dataset)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(X, y):\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    max_len = len(X[0])\n",
    "    for row in X:\n",
    "        if len(row) > max_len:\n",
    "            max_len = len(row)\n",
    "\n",
    "    X = pad_sequences(X, maxlen=max_len, padding='post', dtype='float64')\n",
    "    X = linearize_dataset(X)\n",
    "    return X,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(X,y):\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    max_len = len(X[0])\n",
    "    for row in X:\n",
    "        if len(row) > max_len:\n",
    "            max_len = len(row)\n",
    "\n",
    "    X = pad_sequences(X, maxlen=max_len, padding='post', dtype='float64')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../../datasets/Original/MFCC_5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = TimeSeriesDatasetLoader(DATASET_PATH)\n",
    "X, y = dataset_loader.get_dataset(type_='emotion_type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = balance_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocessing_data(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = discretization(X)\n",
    "X, y = add_padding(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas_profiling import ProfileReport\n",
    "#df = pd.DataFrame(y)\n",
    "#profile = ProfileReport(df, title='Pandas Profiling Report', html={'style':{'full_width':True}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6891301285714289"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traincnn = np.expand_dims(X_train, axis=0)\n",
    "X_testcnn = np.expand_dims(X_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(1, len(X_train), 504)\n",
    "X_test = X_test.reshape(1, len(X_test), 504)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 998 samples, validate on 428 samples\n",
      "Epoch 1/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.1022 - accuracy: 0.2275 - val_loss: 1.0966 - val_accuracy: 0.4112\n",
      "Epoch 2/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0964 - accuracy: 0.4248 - val_loss: 1.0918 - val_accuracy: 0.4112\n",
      "Epoch 3/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0912 - accuracy: 0.4248 - val_loss: 1.0875 - val_accuracy: 0.4112\n",
      "Epoch 4/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0864 - accuracy: 0.4248 - val_loss: 1.0836 - val_accuracy: 0.4112\n",
      "Epoch 5/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0823 - accuracy: 0.4248 - val_loss: 1.0799 - val_accuracy: 0.4112\n",
      "Epoch 6/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0784 - accuracy: 0.4248 - val_loss: 1.0765 - val_accuracy: 0.4112\n",
      "Epoch 7/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0748 - accuracy: 0.4248 - val_loss: 1.0734 - val_accuracy: 0.4112\n",
      "Epoch 8/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0717 - accuracy: 0.4248 - val_loss: 1.0705 - val_accuracy: 0.4112\n",
      "Epoch 9/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0687 - accuracy: 0.4248 - val_loss: 1.0679 - val_accuracy: 0.4112\n",
      "Epoch 10/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0661 - accuracy: 0.4248 - val_loss: 1.0653 - val_accuracy: 0.4112\n",
      "Epoch 11/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0638 - accuracy: 0.4248 - val_loss: 1.0630 - val_accuracy: 0.4112\n",
      "Epoch 12/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0615 - accuracy: 0.4248 - val_loss: 1.0610 - val_accuracy: 0.4112\n",
      "Epoch 13/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0595 - accuracy: 0.4248 - val_loss: 1.0590 - val_accuracy: 0.4112\n",
      "Epoch 14/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0576 - accuracy: 0.4248 - val_loss: 1.0573 - val_accuracy: 0.4112\n",
      "Epoch 15/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0560 - accuracy: 0.4248 - val_loss: 1.0557 - val_accuracy: 0.4112\n",
      "Epoch 16/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0546 - accuracy: 0.4248 - val_loss: 1.0542 - val_accuracy: 0.4112\n",
      "Epoch 17/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0533 - accuracy: 0.4248 - val_loss: 1.0529 - val_accuracy: 0.4112\n",
      "Epoch 18/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0522 - accuracy: 0.4248 - val_loss: 1.0518 - val_accuracy: 0.4112\n",
      "Epoch 19/100\n",
      "998/998 [==============================] - 11s 11ms/step - loss: 1.0512 - accuracy: 0.4248 - val_loss: 1.0507 - val_accuracy: 0.4112\n",
      "Epoch 20/100\n",
      "998/998 [==============================] - 10s 10ms/step - loss: 1.0504 - accuracy: 0.4248 - val_loss: 1.0499 - val_accuracy: 0.4112\n",
      "Epoch 21/100\n",
      "998/998 [==============================] - 10s 10ms/step - loss: 1.0498 - accuracy: 0.4248 - val_loss: 1.0491 - val_accuracy: 0.4112\n",
      "Epoch 22/100\n",
      "998/998 [==============================] - 10s 10ms/step - loss: 1.0492 - accuracy: 0.4248 - val_loss: 1.0485 - val_accuracy: 0.4112\n",
      "Epoch 23/100\n",
      "998/998 [==============================] - 10s 10ms/step - loss: 1.0486 - accuracy: 0.4248 - val_loss: 1.0478 - val_accuracy: 0.4112\n",
      "Epoch 24/100\n",
      "998/998 [==============================] - 11s 11ms/step - loss: 1.0482 - accuracy: 0.4248 - val_loss: 1.0473 - val_accuracy: 0.4112\n",
      "Epoch 25/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0479 - accuracy: 0.4248 - val_loss: 1.0469 - val_accuracy: 0.4112\n",
      "Epoch 26/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0476 - accuracy: 0.4248 - val_loss: 1.0465 - val_accuracy: 0.4112\n",
      "Epoch 27/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0473 - accuracy: 0.4248 - val_loss: 1.0462 - val_accuracy: 0.4112\n",
      "Epoch 28/100\n",
      "998/998 [==============================] - 10s 10ms/step - loss: 1.0473 - accuracy: 0.4248 - val_loss: 1.0460 - val_accuracy: 0.4112\n",
      "Epoch 29/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0469 - accuracy: 0.4248 - val_loss: 1.0457 - val_accuracy: 0.4112\n",
      "Epoch 30/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0469 - accuracy: 0.4248 - val_loss: 1.0455 - val_accuracy: 0.4112\n",
      "Epoch 31/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0467 - accuracy: 0.4248 - val_loss: 1.0453 - val_accuracy: 0.4112\n",
      "Epoch 32/100\n",
      "998/998 [==============================] - 12s 12ms/step - loss: 1.0467 - accuracy: 0.4248 - val_loss: 1.0452 - val_accuracy: 0.4112\n",
      "Epoch 33/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0467 - accuracy: 0.4248 - val_loss: 1.0451 - val_accuracy: 0.4112\n",
      "Epoch 34/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0465 - accuracy: 0.4248 - val_loss: 1.0450 - val_accuracy: 0.4112\n",
      "Epoch 35/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0465 - accuracy: 0.4248 - val_loss: 1.0449 - val_accuracy: 0.4112\n",
      "Epoch 36/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0448 - val_accuracy: 0.4112\n",
      "Epoch 37/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0465 - accuracy: 0.4248 - val_loss: 1.0447 - val_accuracy: 0.4112\n",
      "Epoch 38/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0447 - val_accuracy: 0.4112\n",
      "Epoch 39/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0447 - val_accuracy: 0.4112\n",
      "Epoch 40/100\n",
      "998/998 [==============================] - 8s 8ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0446 - val_accuracy: 0.4112\n",
      "Epoch 41/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0465 - accuracy: 0.4248 - val_loss: 1.0446 - val_accuracy: 0.4112\n",
      "Epoch 42/100\n",
      "998/998 [==============================] - 11s 11ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 43/100\n",
      "998/998 [==============================] - 11s 11ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0446 - val_accuracy: 0.4112\n",
      "Epoch 44/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0446 - val_accuracy: 0.4112\n",
      "Epoch 45/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 46/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 47/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 48/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 49/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 50/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 51/100\n",
      "998/998 [==============================] - 10s 10ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 52/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 53/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0465 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 54/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 55/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 56/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0465 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 57/100\n",
      "998/998 [==============================] - 11s 11ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 58/100\n",
      "998/998 [==============================] - 13s 13ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 59/100\n",
      "998/998 [==============================] - 12s 12ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 60/100\n",
      "998/998 [==============================] - 10s 10ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 61/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 62/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 63/100\n",
      "998/998 [==============================] - 12s 12ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 64/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 65/100\n",
      "998/998 [==============================] - 10s 10ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 66/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0445 - val_accuracy: 0.4112\n",
      "Epoch 67/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 68/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 69/100\n",
      "998/998 [==============================] - 11s 11ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 70/100\n",
      "998/998 [==============================] - 14s 14ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 71/100\n",
      "998/998 [==============================] - 15s 15ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 72/100\n",
      "998/998 [==============================] - 11s 11ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 73/100\n",
      "998/998 [==============================] - 13s 13ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 74/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 75/100\n",
      "998/998 [==============================] - 10s 10ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 76/100\n",
      "998/998 [==============================] - 8s 8ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 77/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 78/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 79/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 80/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 81/100\n",
      "998/998 [==============================] - 8s 8ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 82/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 83/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 84/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 85/100\n",
      "998/998 [==============================] - 8s 8ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 86/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 87/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0465 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 88/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 89/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 90/100\n",
      "998/998 [==============================] - 9s 9ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0443 - val_accuracy: 0.4112\n",
      "Epoch 91/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0443 - val_accuracy: 0.4112\n",
      "Epoch 92/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 93/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 94/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 95/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 96/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 97/100\n",
      "998/998 [==============================] - 8s 8ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 98/100\n",
      "998/998 [==============================] - 8s 8ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 99/100\n",
      "998/998 [==============================] - 7s 7ms/step - loss: 1.0463 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n",
      "Epoch 100/100\n",
      "998/998 [==============================] - 6s 6ms/step - loss: 1.0464 - accuracy: 0.4248 - val_loss: 1.0444 - val_accuracy: 0.4112\n"
     ]
    }
   ],
   "source": [
    "embedding_size  = 128\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1, embedding_size, input_length=504))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=['accuracy'])\n",
    "cnnhistory=model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 500\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt, metrics=['accuracy'])\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=20, min_lr=0.0000001)\n",
    "mcp_save = ModelCheckpoint('../model_checkpoints/two_split.h5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_5 to have shape (1,) but got array with shape (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-40554bcf0dbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnnhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# , callbacks=[mcp_save, lr_reduce]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_5 to have shape (1,) but got array with shape (3,)"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, y_test))\n",
    "# , callbacks=[mcp_save, lr_reduce]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 504, 128)          768       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 504, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 504, 64)           41024     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 504, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 504, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 63, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 63, 32)            10272     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 63, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2016)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 6051      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 58,115\n",
      "Trainable params: 58,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  6.404657  ,  -1.402514  ,  -2.236425  , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.8584499 ,  -0.06234932,   0.6381655 , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  5.846846  , -10.07883   , -11.02912   , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        ...,\n",
       "        [ 12.99762   ,  -7.800581  ,  -1.155792  , ...,  -0.5558    ,\n",
       "          -1.747546  ,   3.087212  ],\n",
       "        [  3.280588  ,   2.007889  ,  -2.019495  , ...,  -0.9734643 ,\n",
       "           0.8074539 ,   2.71006   ],\n",
       "        [  5.084667  ,  -3.900648  ,  12.28177   , ...,  -0.7672844 ,\n",
       "           2.181755  ,   4.326027  ]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' More Preprocessing '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scalers = {}\n",
    "for i in range(X.shape[1]):\n",
    "    scalers[i] = RobustScaler()\n",
    "    X[:, i, :] = scalers[i].fit_transform(X[:, i, :]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scalers = {}\n",
    "for i in range(X.shape[1]):\n",
    "    scalers[i] = Normalizer()\n",
    "    X[:, i, :] = scalers[i].fit_transform(X[:, i, :]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.28221284,  0.08020835, -0.09187018,\n",
       "       -0.10788761,  0.62792463,  0.3675004 ,  0.16599244,  0.18636285,\n",
       "       -0.35420188,  0.19850305, -0.14629544, -0.18601133, -0.21646694,\n",
       "        0.18571278])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 502992 into shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a444714b1a9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 502992 into shape (1,)"
     ]
    }
   ],
   "source": [
    "X_train.reshape(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Finishing Preprocessing '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See X and y details\n",
    "print(X[:2])\n",
    "print(X.shape)\n",
    "\n",
    "print(y[:3])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train:\n",
      "\n",
      "9.105775999999999\n",
      "(1364, 504)\n",
      "\n",
      "X_test:\n",
      "\n",
      "-0.1714754\n",
      "(585, 504)\n",
      "\n",
      "y_train:\n",
      "\n",
      "[2 1 2]\n",
      "(1364,)\n",
      "\n",
      "y_test:\n",
      "\n",
      "[1 2 2]\n",
      "(585,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset in train and test\n",
    "\n",
    "print(\"\\nX_train:\\n\")\n",
    "print(X_train[0][0])\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"\\nX_test:\\n\")\n",
    "print(X_test[0][0])\n",
    "print(X_test.shape)\n",
    "\n",
    "print(\"\\ny_train:\\n\")\n",
    "print(y_train[:3])\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"\\ny_test:\\n\")\n",
    "print(y_test[:3])\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9ae14aac2254>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnum_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "num_rows = X[0].shape[0]\n",
    "num_columns = X[0].shape[1]\n",
    "num_channels = 1\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], num_rows, num_columns, num_channels)\n",
    "X_test = X_test.reshape(X_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "print(\"\\nX_train:\\n\")\n",
    "print(X_train[0][0])\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"\\nX_test:\\n\")\n",
    "print(X_test[0][0])\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gabriel/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-k category accuracy\n",
    "import functools\n",
    "import keras\n",
    "top3_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=3)\n",
    "top3_acc.__name__ = 'top3_acc'\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',top3_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv1d_3_input to have 3 dimensions, but got array with shape (1364, 504)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f7b1a1c48611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlr_reduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0000001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmcp_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../models/model_checkpoints/two_split.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcnnhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmcp_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_reduce\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/machine-learning/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv1d_3_input to have 3 dimensions, but got array with shape (1364, 504)"
     ]
    }
   ],
   "source": [
    "# Define bath and epochs\n",
    "batch_size = 256\n",
    "epochs = 400\n",
    "checkpoint_file = '../models/model_checkpoints/original_window_2.h5'\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',top3_acc])\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=20, min_lr=0.0000001)\n",
    "mcp_save = ModelCheckpoint('../models/model_checkpoints/two_split.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "cnnhistory=model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_test, y_test), callbacks=[mcp_save, lr_reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6239316463470459\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = cnnhistory.history['val_accuracy']\n",
    "highest_index = cnnhistory.history['val_accuracy'].index(np.sort(cnnhistory.history['val_accuracy'])[-1])\n",
    "print(cnnhistory.history['val_accuracy'][highest_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without prepocessing: 0.5863248109817505\n",
    "# robust preprocessing: 0.6034188270568848\n",
    "# normalization: 0.6239316463470459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model Summary\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# PLT History info\n",
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
